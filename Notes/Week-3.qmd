---
title: "Week 3 Notes: GLMs in Practice"
format:
  pdf:
    include-in-header:
      - file: macros.tex
    keep-tex: true
number-depth: 1
number-sections: true
---

This week, we'll mainly be doing some open-ended applications of generalized
linear models on real datasets. Some of them may involve doing some independent
work on your own in figuring out how to fit the models. For each question, I
would like you to consider the following issues:

- What modeling strategy is appropriate for the data?

# Problem 1: Estimating a Generalized Linear Model from Scratch

A common approach to fitting GLMs via maximum likelihood (or, really, optimizing
*any* function) is to use *gradient descent*, i.e., iteratively update
\begin{align*}
  \beta \gets \beta + \gamma \, s(\beta)
\end{align*}
where $s(\cdot)$ is the score function and $\gamma$ is a "learning rate" that
is, ideally, selected automatically. Read up on this topic\thanks{if you want a
textbook reference, see \emph{Numerical Optimization} by Nocedal and Wright} and
write your own function that will fit a logistic regression model by gradient
descent. For extra points, try to maintain some level of generality in your
code, i.e., so that it could also work with different GLMs if you wrote
different subroutines.

After doing this, grab the data `wdbc.csv` from the course website, or obtain
some other real data that interests you, and test out your routine (comparing it
to the output of, say, `glm` in `R` to sanity check your results). The WDBC file
has information on 569 breast-cancer patients from a study done in Wisconsin.
The first column is a patient ID, the second column is a classification of a
breast cell (Malignant or Benign), and the next 30 columns are measurements
computed from a digitized image of the cell nucleus. These are things like
radius, smoothness, etc. For this problem, use the first 10 features for X, i.e.
columns 3-12 of the file.

**Some points for discussion**

- How did you select the learning rate $\gamma$? You can be as clever (or as
  non-clever) as you want here. Generally, smaller step sizes will be more
  robust, but converge slower, while larger step sizes may overshoot the
  solution entirely.
  
- Ensure that, at every iteration of gradient descent, you compute and store the
  current value of the log-likelihood so that you can track and plot the
  convergence of the algorithm.

- How do you determine (automatically) that you have reached a reasonable
  stopping point? One possibility is to stop when $s(\beta)$ is "small enough".

# Problem 2: Estimating a Generalized Linear Model from Scratch: 2nd Order Methods

Rather than using 
