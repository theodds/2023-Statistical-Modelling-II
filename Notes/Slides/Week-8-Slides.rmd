---
title: "Week 8 Notes: Some Nonparametric Methods"
author: | 
  | Antonio R. Linero
  | University of Texas at Austin
output:
  beamer_presentation:
    includes:
      in_header: "preamble.tex"
classoption:
  - "xcolor=svgnames,dvipsnames,html"
keep_tex: yes
urlcolor: blue
---

# Goals

1. Be able to perform linear smoothing, including selecting bandwidth parameters.

1. Know different linear smoothing techniques (local constant, local linear,
   basis function expansions).
   
1. Be able to do Bayesian inference with Gaussian processes.

1. Know the fundamental difficulties with nonparametric estimation.

# Nonparametrics

\footnotesize

\alert{Recall the differences between parametric, semiparametric, and
nonparametric methods.}

\pause

\vspace{1em}

- **Parametric:** DGP indexed by finite dimensional $\theta$. (Normal linear
  regression, most GLMs).

- **Semiparametric:** DGP indexed by finite-dimensional parameter of interest $\theta$ a an infinite-dimensional nuisance parameter $\eta$. (GEEs, Quasi-Likelihood, Empirical Likelihood)

- **Nonparametric:** DGP indexed by infinite-dimensional $\theta$ of interest.

\vspace{1em}

\alertb{Note: some flexibility in the terms.}

# Bias-Variance Tradeoff

\footnotesize

:::{.myexercise data-latex="[Bias-Variance Tradeoff]"}

Show that $\MSE(\widehat\mu, \mu)$ decomposes into a _bias_ term and a
_variance_ term $\MSE = B^2 + V$:
$$
  B = \E_\theta\{\widehat\mu(x)\} - \mu(x)
  \qquad \text{and} \qquad
  V = \Var\{\widehat\mu(x)\}.
$$

:::

\vspace{1em}

\alert{Typical situation:} in nonparametric settings:

- Estimation error is balanced when *these terms are of the same magnitude.*
- Uncertainty quantification requires *the variance term to dominate.* (Called
  *undersmoothing*.)

# Density Estimation With Histograms

\tiny

:::{.myexercise data-latex="[Density Estimation With Histograms]"}

Some people refer to the decomposition above as the _bias-variance tradeoff_.
Why is this a tradeoff? Here's a simple example to convey the intuition.

Suppose we observe $Z_1, \ldots, Z_N$ from some distribution $F$ and want to
estimate $f(0)$, the value of the probability density at $0$. Let $h$ be a small
positive number, called the _bandwidth_, and define the quantity
$$
  \pi_h 
  = \Pr\left( -\frac{h}{2} < Z < \frac{h}{2}\right)
  = \int_{-\frac{h}{2}}^{\frac{h}{2}} f(z) \ dz.
$$
For small $h$ we have $\pi_h \approx h \, f(0)$ provided that $f(x)$ is
continuous at $0$.

a. Let $M$ be the number of observations in a sample of size $N$ that fall
   within the interval $(-h/2, h/2)$. What is the distribution of $M$. What are
   its mean and variance in terms of $N$ and $\pi_h$? Propose a simple estimator
   $\widehat f(0)$ of $f(0)$ based on $M$.

b. Suppose we expand $f(z)$ in a second-order Taylor series about $0$:
   $$
     f(z) \approx f(0) + f'(0) \, z + \frac{1}{2} f''(0) \, z^2.
   $$
   Use this, together with the bias-variance decomposition, to show that
   $$
     \MSE\{\widehat f(0), f(0)\}
     \approx
     A h^4 + \frac{B}{Nh}
   $$
   for constants $A$ and $B$ that you should (approximately) specify. What
   happens to the bias and variance when you make $h$ small? When you make $h$
   big?

c. Use this result to derive an expression for the bandwidth that minimizes the
   mean-squared error as a function of $N$. You can approximate any constants that
   appear, but make sure you get the right functional dependence on the sample
   size.

:::

# Curve Fitting by Linear Smoothing

\tiny

:::{.myexercise data-latex="[Linear Smoothing]"}

Consider a nonlinear regression problem with one predictor and one response:
$Y_i = \mu(X_i) + \epsilon_i$ where the $\epsilon_i$'s are mean-zero random
variables.

a. Suppose you want to estimate the value of the regression function
   $\mu(x^\star)$ at some new point $x^\star$. Assume for the moment that
   $\mu(x)$ is linear and that both $Y_i$ and $X_i$ are mean $0$, in which case
   $Y_i = \beta \, X_i + \epsilon_i$.

   Recall the least-squares estimator for multiple regression. Show that for the
   one-predictor case, your prediction $\mu(x^\star) = \widehat \beta \,
   x^\star$ can be expressed as a _linear smoother_ of the form
   $$
     \widehat \mu(x^\star)
     =
     \sum_{i=1}^N w(X_i, x^\star) \, Y_i
   $$
   for any $x^\star$. Inspect the weighting function you derived. Briefly
   describe your understanding of how the resulting smoother behaves, compared
   with the smoother that arises from an alternate form of the weight function
   $w(X_i, x^\star)$:
   $$
     w_K(X_i, x^\star) = 
     \begin{cases}
       1 / K \qquad & 
         \text{if $X_i$ is one of the $K$ closest sample points to $x^\star$},  \\
       0 \qquad & \text{otherwise}.
     \end{cases}
   $$
   This is referred to as the _K-nearest neighbor_ smoother.
    
b. A _kernel function_ $K(x)$ is a smooth function satisfying
   $$
     \int_{-\infty}^\infty K(x) \ dx = 1, 
     \qquad
     \int_{-\infty}^\infty x \, K(x) \ dx = 0.
     \qquad
     \int_{-\infty}^\infty x^2 \, K(x) \ dx < \infty.
   $$
   A very simple example is the uniform kernel,
   $$
     K(x) = \frac{1}{2} \, I(x) \qquad \text{where} \qquad
     I(x) = \begin{cases}
       1 \qquad & \text{if $|x| \leq 1$}, \\
       0 \qquad & \text{otherwise}.
     \end{cases}
   $$
   Another common choice is the _Gaussian kernel_,
   $$
     K(x) = \frac{1}{\sqrt{2 \pi}} \, \exp\left(-\frac{x^2}{2}\right).
   $$
   Kernels are used as weighting functions for taking local averages;
   specifically, define the weighting function
   $$
     w(X_i, x^\star) = \frac{1}{h}K\left(\frac{X_i - x^\star}{h}\right),
   $$
   where $h$ is the bandwidth. Using this weighting function in a linear
   smoother is called _kernel regression_. (The weighting function gives
   unnormalized weights; you should also normalize the weights so that they sum
   to $1$.)

   __Write your own `R` function that will fit a kernel smoother for an
   arbitrary set of $(X_i, Y_i)$ pairs and arbitrary choice of (positive real)
   bandwidth $h$, and test it on a nonlinear function of your choice.__ You may
   choose your own kernel. Set up an `R` script that will simulate noisy data
   from some nonlinear function $Y_i = \mu(X_i) + \epsilon_i$; subtract the
   sample means from the simulated data; and use your function to fit the kernel
   smoother for some choice of $h$. Plot the estimated functions for a range of
   bandwidths wide enough to yield noticable differences in the qualitative
   behavior of the prediction functions.

::::

# Cross-Validation for Selecting Bandwidths

\tiny

:::{.myexercise data-latex="[Cross Validation]"}

Left unanswered so far in our previous study of kernel regression is the
question: how does one choose the bandwidth $h$ used for the kernel? Assume for
now that the goal is to predict well, not necessarily to recover the truth.
(These are related but distinct goals.)

a. Presumably a good choice of $h$ would be one that led to smaller predictive
   errors on fresh data. Write a function or script that will: (1) accept an old
   ("training") data set and a new ("testing") data set as inputs; (2) fit the
   kernel-regression estimator to the training data for specified choices of
   $h$; and (3) return the estimated functions and the realized prediction error
   on the testing data for each value of $h$. This should involve a fairly
   straightforward "wrapper" of the function you've already written.

b. Imagine a conceptual two-by-two table for the unknown, true state of affairs.
   The rows of the table are "wiggly function" and "smooth function," and the
   columns are "highly noisy observations" and "not so noisy observations."
   Simulate one data set (say, 500 points) for each of the four cells of this
   table, where the $X_i$'s take values in the unit interval. Then split each
   data set into training and testing subsets. You choose the
   functions.^[Trigonometric functions, for example, can be pretty wiggly if you
   make the period small.] Apply your method to each case, using the testing data
   to select a bandwidth parameter. Choose the estimate that minimizes the
   average squared error in prediction, which estimates the mean-squared error:
   $$
     L_N(\widehat{\mu}) = \frac{1}{N^\star}\sum_{i=1}^{N^{\star}}
       (Y^{\star}_i - \widehat{Y}_i^{\star} )^2 \, ,
   $$
   where $(Y_i^{\star}, X_i^{\star})$ are the points in the test set, and
   $\widehat{Y}_i^{\star}$ is your predicted value arising from the model you
   fit using only the training data. Does your out-of-sample predictive
   validation method lead to reasonable choices of $h$ for each case?

c. __Optional: ("leave-one-out lemma)"__ Splitting a data set into two chunks to
   choose $h$ by out-of-sample validation has some drawbacks. (See if you can think
   of two; one is obvious, one is more subtle.) Then consider an alternative:
   leave-one-out cross validation. Define
   $$
     \mbox{LOOCV} = \sum_{i=1}^N \left( Y_i - \widehat{Y}_{i}^{(-i)} \right)^2 \, ,
   $$
   where $\widehat{Y}_{i}^{(-i)}$ is the predicted value of $Y_i$ obtained by
   omitting the $i^{\text{th}}$ pair $(X_i, Y_i)$ and fitting the model to the
   training data set of size $N - 1$.^[The intuition here is straightforward: for
   each possible choice of $h$, you have to predict each data point using all
   the others. The bandwidth that with the lowest prediction error is the "best"
   choice by the LOOCV criterion.] This $\widehat{Y}_{i}^{(-i)}$ is contingent
   upon a particular bandwidth, and is obviously a function of $X_i$, but these
   dependencies are suppressed for ease of notation. This error metric looks
   expensive to compute: for each value of $h$, and for each data point to be
   held out, it seems as though you must fit a whole nonlinear regression model.
   But happily, there's a short-cut!

   Observe that for a linear smoother, we can write the whole vector of fitted
   values as $\widehat{Y} = H Y$, where $H$ is called the smoothing matrix (or
   "hat matrix") and $Y$ is the vector of observed outcomes. Deduce that, for
   any linear smoother,
   $$
   \mbox{LOOCV} = \sum_{i=1}^N
     \left( \frac{Y_i - \widehat{Y}_{i} } {1-H_{ii}} \right)^2 \, .
   $$

   _Hint:_ For simplicity, you may assume that the weights $w(X_i, X_j)$ are such
   that $\sum_j w(X_i, X_j) = 1$, and that the held-out prediction is also formed
   by normalizing the weights; in this case, to normalize the weights we would
   use the new weights $w^\star(X_i, X_j) = \frac{w(X_i, X_j)}{1 - w(X_i, X_i)}$ so
   that $\sum_{j \ne i} w^\star(X_i, X_j) = 1$; alternatively, we could define the
   weights to include observation $i$ by writing $w^\star(X_i, X_j) = \frac{w(X_i,
   X_j) \, \{1 - I(i = j)\}}{1 - w(X_i, X_i)}$. Using this, you should be able to
   show directly that $Y_i - \widehat Y^{(-i)}_i = \sum_j w^\star(X_i, X_j) \, (Y_i -
   Y_j) = \frac{Y_i - \widehat Y_i}{1 - w(X_i, X_i)}$.

d. Use the leave-one-out lemma to revisit the examples you simulated in Part B,
   using leave-one-out cross validation to select $h$ in each case. Because of
   the leave-one-out lemma, you won't need to actually refit the model N times!

:::

# Heteroskedasticity

\tiny

:::{.myexercise data-latex="[Heteroskedasticity]"}

In this exercise we will consider linear smoothing when (potentially) we are
concerned that the errors in the model $Y_i = r(X_i) + \epsilon_i$ do not have
constant variance.

a. Suppose that the $\epsilon_i$'s have constant variance $\sigma^2$ (that is,
   the spread of the residuals does not depend on $x$). Derive the mean and
   variance of the sampling distribution for the locally constant linear
   smoother. Note: the random variable $\widehat{\mu}(x)$ is just a scalar
   quantity at $x$, not the whole function.

b. We don't know the residual variance, but we can estimate it. A basic fact is
   that if $X$ is a random vector with mean $\mu$ and covariance matrix
   $\Sigma$, then for any symmetric matrix $Q$ of appropriate dimension, the
   quadratic form $X^\top Q X$ has expectation
   $$
     E(X^\top Q X) = \mbox{tr}(Q \Sigma) + \mu^\top Q \mu \, .
   $$
   Consider an arbitrary linear smoother (i.e., one with $\widehat Y = HY$ for
   some smoothing matrix $H$). Write the vector of residuals as $R = Y -
   \widehat{Y} = Y - HY$, where $H$ is the smoothing matrix. Compute the
   expected value of the estimator
   $$
     \widehat{\sigma}^2
     = \frac{\|R\|^2}{n - 2\mbox{tr}(H) + \mbox{tr}(H^\top H)} \, ,
   $$
   and simplify things as much as possible. Roughly under what circumstances
   will this estimator be nearly unbiased for large $N$? Note: the quantity
   $2\mbox{tr}(H) - \mbox{tr}(H^\top H)$ is often referred to as the "effective
   degrees of freedom" in such problems.

c. Load `utilities.csv` located at
   https://raw.githubusercontent.com/theodds/StatModelingNotes/master/datasets/utilities.csv
   into `R`. This data set shows the monthly gas bill (in dollars) for a
   single-family home in Minnesota, along with the average temperature in that
   month (in degrees F), and the number of billing days in that month. Let $Y_i$
   be the average daily gas bill in a given month (i.e. dollars divided by
   billing days), and let $X_i$ be the average temperature. Using leave-one-out
   cross-validation to choose the bandwidth, make a scatterplot of `gasbill` as
   a function of `temp` with a kernel smooth of the relationship overlayed.

d. Inspect the residuals from the model you just fit.  Does the assumption of
   constant variance (homoskedasticity) look reasonable? If not, do you have any
   suggestion for fixing it?

e. Put everything together to construct an approximate point-wise 95\%
   confidence interval for your kernel smoother (using your chosen bandwidth)
   for the value of the function at each of the observed points $X_i$ for the
   utilities data. Plot these confidence bands, along with the estimated
   function, on top of a scatter plot of the data.^[It's fine to use Gaussian
   critical values for your confidence set.]

:::

# Locally Constant Regression

\footnotesize

- \alert{Kernel smoothing:} **locally constant**, solves
  $$
  \widehat\mu(x) = a = 
  \arg \min_{\mathbb R} \sum_{i=1}^N w_i(x) (Y_i - a)^2 \, ,
  $$
https://rafalab.dfci.harvard.edu/dsbook/ml/img/binsmoother-animation.gif
\pause \vspace{1em}

- \alert{Local linear smoothing:} use a **local polynomial** instead
  $$
    \widehat\mu(x) = x^\top \beta(x) =
    x^\top \arg \min_{\beta} \sum_{i=1}^N w_i(x) (Y_i - X_i^\top \beta)^2
  $$
  https://rafalab.dfci.harvard.edu/dsbook/ml/img/loess-animation.gif

---

# Loess

- Tricube weighting function
  $$
    w_i(x) = \left\{1 - \left(\frac{|x - X_i|}{h_\alpha(x)}\right)^3\right\}_+^3
  $$

- Data-adaptive bandwidth:
  $$
    h_\alpha(x) = \max_{j: X_j \in B_x}\{|x - X_j|\}
  $$
  where $B_x$ is a neighborhood of $x$ chosen so that $100\alpha\%$ of the
  $X_i$'s are in $B_x$.

# Example

\tiny

```{r setup, include = FALSE}
library(tidyverse)
theme_set(theme_bw())
```

```{r code-show, eval = FALSE}
utilities_file <- str_c("https://raw.githubusercontent.com/theodds",
                        "/StatModelingNotes/master/datasets/utilities.csv")
utilities <- readr::read_csv(utilities_file)
loess_util <- loess(gasbill ~ temp, data = utilities, span = 0.2, degree = 1)
loess_preds <- predict(loess_util, utilities, se = TRUE)
utilities %>%
  mutate(fit = loess_preds$fit, se = loess_preds$se.fit) %>%
  ggplot(aes(x = temp, y = gasbill, ymin = fit - 2 * se, ymax = fit + 2 * se)) +
  geom_point() + 
  geom_ribbon(alpha = 0.3, fill =   "#0072B2") + 
  geom_line(aes(y = fit), color = "#E69F00", size = 2)
```

# Example

```{r code-eval, echo = FALSE, message = FALSE}
utilities_file <- str_c("https://raw.githubusercontent.com/theodds",
                        "/StatModelingNotes/master/datasets/utilities.csv")
utilities <- readr::read_csv(utilities_file)
loess_util <- loess(gasbill ~ temp, data = utilities, span = 0.2, degree = 1)
loess_preds <- predict(loess_util, utilities, se = TRUE)
utilities %>%
  mutate(fit = loess_preds$fit, se = loess_preds$se.fit) %>%
  ggplot(aes(x = temp, y = gasbill, ymin = fit - 2 * se, ymax = fit + 2 * se)) +
  geom_point() + 
  geom_ribbon(alpha = 0.3, fill =   "#0072B2") + 
  geom_line(aes(y = fit), color = "#E69F00", size = 2)
```

# Bayesian Variant: Gaussian Processes

\footnotesize

Choose a prior on a \alert{function space} $\mathscr F$, with large support:
$$
  \Pi(\sup_{x \in \mathcal X} |\mu_0(x) - \mu(x)| < \epsilon) > 0
$$

\pause

**Examples of spaces:**

- $C^0(\mathcal X)$
- $C^2(\mathcal X)$
- $\mathscr L_2(\mathcal X)$

# Gaussian Processes

\footnotesize

:::{.definition data-latex="[Gaussian Process]"}

Let $m : \mathcal X \to \mathbb R$ and $K: \mathcal X^2 \to \mathbb R$. A random
function $\mu : \mathcal X \to \mathbb R$ is said to be a _Gaussian process_ if,
for any _finite_ set $D = \{x_1, \ldots x_D\}$ we have
$$
  \mu(\mathbf x) =
  \Normal\{m(\mathbf x), K(\mathbf x, \mathbf x)\}
$$
where $\mathbf x = (x_1, \ldots, x_D)^\top$, $\mu(\mathbf x) = (\mu(x_1),
\ldots, \mu(x_D))^\top$, $m(\mathbf x) = (m(x_1), \ldots, m(x_D))^\top$, and
$K(\mathbf x, \mathbf x')$ is a covariance matrix with $(i,j)^{\text{th}}$ entry
$K(x_i, x'_j)$. The function $K(\cdot, \cdot)$ is referred to as a _covariance
function_. To denote this fact, we write $\mu \sim \GP(m, K)$.

:::

\alert{When is $K(\mathbf x, \mathbf x')$ valid?}

# Review of MVN

:::{.definition data-latex="[Multivariate Normal Distribution]"}

We say that $X$ has an _$n$-dimensional multivariate normal_ distribution with
mean vector $\mu$ and covariance matrix $\Sigma$ if, or every $\lambda \in
\Reals^n$, we have $\lambda^\top X \sim \Normal(\lambda^\top \mu, \lambda^\top
\Sigma \lambda)$. We write $X \sim \Normal(\mu, \Sigma)$ to denote this fact;
this distribution exists for every $\mu \in \mathbb R^n$ and every symmetric
matrix $\Sigma \in \mathbb R^{n \times n}$ such that $\lambda^\top \Sigma
\lambda \ge 0$ for all $\lambda \in \mathbb R^n$ (such a matrix is called
_positive semi-definite_).

:::

# Exercise

:::{.myexercise data-latex="[Mean and Variance]"}

Show that $\mu = \E(X)$ and $\Sigma = \Var(X)$ from this definition.

:::

# Exercise

:::{.myexercise data-latex="[Characteristic Functions]"}

The _characteristic function_ of a random vector $X$ is the function
$$
\varphi_X(\lambda)
  = \E(e^{i\lambda^\top X})
$$
where $i$ is the imaginary unit (i.e., $i^2 = -1$). The characteristic function
is similar to the moment generating function, with the important benefit that
$\varphi_X(\lambda)$ is guaranteed to exist for all $\lambda$. It can be shown
that, if $X$ and $Y$ have the same characteristic function, then $X$ and $Y$
have the same distribution (the proof is not difficult if you are comfortable
with real analysis, but not worth our time; see [this
website](http://theanalysisofdata.com/probability/8_8.html) for all the
ingredients of the proof).

a. Show that the multivariate normal distribution is "well-defined" by
   Definition; that is, show that if $X$ and $Y$ both satisfy
   Definition then they have the same distribution. Why do we
   require that $\Sigma$ is positive semi-definite and symmetric?

b. Let $Z = (Z_1, \ldots, Z_n)$ be such that $Z_i \iid \Normal(0, 1)$ and let $Y
   = \mu + L Z$. Show that $Y \sim \Normal(\mu, L L^\top)$.

c. Show that this also holds in the other direction: if $Y \sim \Normal(\mu,
   \Sigma)$ then there exists a matrix $L$ such that $Y \stackrel{d}{=} \mu + L
   Z$ where $Z = (Z_1, \ldots, Z_n) \iid \Normal(0,1)$ (where $\stackrel{d}{=}$
   denotes equality in distribution). Based on this fact, propose an algorithm
   for randomly generating $Y \sim \Normal(\mu, \Sigma)$. _Hint:_ use either the
   Cholesky decomposition or the eigen decomposition to construct a matrix $L$
   such that $LL^\top = \Sigma$.

:::

# Exercise

:::{.myexercise data-latex="[Density of a MVN]"}

Suppose that $\Sigma$ is full-rank and that $Y \sim \Normal(\mu, \Sigma)$. Show
that $Y$ has density
$$
  f(y \mid \mu, \Sigma)
  =
  \frac{1}{\sqrt{(2\pi)^n |\Sigma|}}
    \exp\left\{ -\frac 1 2 (y - \mu)^\top \Sigma^{-1} (y - \mu) \right\}.
$$
What happens if $\Sigma$ is _not_ full-rank? _Hint:_ recall the change of
variables formula, which states that if $Y = T(Z)$ where $T$ is a smooth
one-to-one function then the density of $Y$ is
$$
  f_Y(y) = f_Z\{T^{-1}(y)\} |J(y)|
$$
 where $J(y)$ is the Jacobian matrix of $T^{-1}$. When $T(Z) = \mu + L Z$ where
 $L$ is a full-rank matrix, this simplifies to
$$
  f_Y(y) = f_Z\{L^{-1}(Y - \mu)\} |L^{-1}|. 
$$

:::

# Exercise

:::{.myexercise data-latex="[Characteristic Function of a MVN]"}

We now derive the characteristic function of the multivariate normal
distribution.

a. Show that a standard normal random variable $Z$ has characteristic function
   $$
     \varphi_Z(t)
     =
     \int \cos(tz) \, e^{-z^2 / 2} \ dz
     =
     e^{-t^2 / 2}.
   $$
   This can be done in two steps: (i) because $\sin(tZ)$ is an odd function and
   $Z$ is symmetric, the imaginary part disappears and (ii) by differentiating
   under the integral, we can establish the differential equation $\frac d {dt}
   \varphi_Z(t) = -t \varphi_Z(t)$; solving this equation with the initial
   condition $\varphi_Z(0) = 1$ gives the result.
   
b. Using this result, show that the characteristic function of $X \sim
   \Normal(\mu, \Sigma)$ is $$ \varphi_X(\lambda) = \exp\left(i \lambda^\top \mu
   - \frac{\lambda^\top\Sigma\lambda}{2}\right). $$

:::

# MVN Properties

\footnotesize

The multivariate normal distribution has a large number of desirable properties.
First, it is _closed under marginalization_. Suppose that
$$
  \begin{pmatrix}
    X \\ Y
  \end{pmatrix}
  \sim
  \Normal \left(
  \begin{pmatrix}
    \mu_x \\ \mu_y
  \end{pmatrix},
  \begin{pmatrix}
    \Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy}.
  \end{pmatrix}
  \right).
$$
Here, the vectors $\mu_x$ and $\mu_y$ have the same dimension as $X$ and $Y$,
and $\Sigma_{xx}$ and $\Sigma_{yy}$ are positive semi-definite symmetric
matrices with dimensions matching $X$ and $Y$ respectively. Because the
covariance matrix of $(X,Y)$ is symmetric, it follows that $\Sigma_{xy} =
\Sigma_{yx}^\top$.


# Exercise

\tiny

:::{.myexercise data-latex="[MVN Properties]"}

We now prove some basic properties.

a. Show that $X \sim \Normal(\mu_x, \Sigma_{xx})$.

b. The _covariance_ of $X$ and $Y$ is defined to be $\Cov(X,Y) = \E\{(X -
   \mu_x)(Y - \mu_y)^\top\}$. Show that (i) for any random vectors, if $X$ and
   $Y$ are independent then the covariance is equal to the zero matrix and (ii)
   for the multivariate normal distribution in particular the covariance matrix
   is $\Sigma_{xy}$.

c. Using the characteristic function, show that for the multivariate normal
   distribution $X$ is independent of $Y$ if-and-only-if $\Sigma_{xy}$ is equal
   to zero. This is an interesting reversal -- in general, the covariance being
   $0$ does not imply independence, but it does for multivariate normal random
   vectors.

d. Suppose $Y \sim \Normal(\mu, A)$ given $\mu$ and $\mu \sim \Normal(m, B)$.
   Show that
   $$
   \begin{aligned}
   \begin{pmatrix}
     Y \\ \mu
   \end{pmatrix}
   &\sim
     \Normal\left\{
     \begin{pmatrix}
       m \\ m
     \end{pmatrix},
   \begin{pmatrix}
     A + B & B \\
     B & B
   \end{pmatrix}
     \right\}.
   \end{aligned}
   $$
:::

# Exercise

\tiny

:::{.myexercise data-latex="[The Conditionals]"}

We will now show that the conditional distribution of $X$ given $Y = y$ is
\begin{align}
    \label{eq:conditional}
    X \sim \Normal(\mu_{x \mid y}, \Sigma_{x \mid y})
\end{align}
where $\mu_{x \mid y} = \mu_x + \Sigma_{xy} \Sigma_{yy}^{-1} (y - \mu_y)$ and
$\Sigma_{x \mid y} = \Sigma_{xx} - \Sigma_{xy} \Sigma_{yy}^{-1} \Sigma_{yx}$.

a. Write $X = W + (X - W)$ where $W = \mu_x + \Sigma_{xy} \Sigma_{yy}^{-1} (Y -
   \mu_y)$. Show that $\Cov(Y, X - W) = 0$ so that $X - W$ is independent of
   $Y$.

b. Show that the covariance matrix of $X - W$ is $\Sigma_{xx} - \Sigma_{xy}
   \Sigma_{yy}^{-1} \Sigma_{yx}$ and that the mean is $\boldsymbol 0$.

c. Argue that because (i) $W$ is constant as a function of $Y$, (ii) $X - W$ is
   independent of $Y$, (iii) $X = W + (X - W)$, and (iv) $X - W \sim
   \Normal(\boldsymbol 0, \Sigma_{xx} - \Sigma_{xy} \Sigma_{yy}^{-1}
   \Sigma_{yx})$ that we can conclude that the distribution of $X$ given $Y$ is
   given by \eqref{eq:conditional}.

:::

# Exercise

\footnotesize

:::{.myexercise data-latex="[GP Inference]"}

Suppose that $Y_i \stackrel{\text{indep}}{\sim} \Normal\{\mu(X_i), \sigma^2\}$
conditional on $\bX$ for $i = 1, \ldots, N$ and $\mu$. Let $\bX = (X_1, \ldots,
X_N)$ and $\bY = (Y_1, \ldots, Y_N)$.

a. Show that the posterior distribution of $\mu$ is given by
$$
  [\mu \mid \bX, \bY, \sigma^2]
  \sim
  \GP(m^\star, K^\star)
$$
where
$$
\begin{aligned}
  m^\star(x) &=
    m(x) +
      K(x, \bX) \{K(\bX, \bX) +
      \sigma^2 \Identity\}^{-1} \{\mathbf Y - m(\mathbf X)\} \\
  K^\star(x, x') &= 
    K(x, x') 
      -K\left(\binom{x}{x'}, \bX\right) 
      \{K(\bX, \bX) + \sigma^2 \Identity\}^{-1} 
      K\left(\binom{x}{x'}, \bX\right)^\top
\end{aligned}
$$

b. Argue that the marginal likelihood of $\bY$ (i.e., with the random function
   $\mu$ integrated out) is given by
   $$
     |2\pi(K(\bX, \bX) + \sigma^2 \, \Identity)|^{-1/2} 
     \exp\left\{-\frac 1 2 (\bY - m(\bX))^\top 
                          (K(\bX, \bX) + \sigma^2 \, \Identity)^{-1} 
                          (\bY - m(\bX))\right\}.
   $$
   Or, equivalently, that $[\bY \mid \bX, \sigma] \sim \Normal\left\{m(\bX),
   K(\bX, \bX) + \sigma^2 \, \Identity\right\}$.

:::

# Exercise

:::{.myexercise data-latex="[Squared Exponential Kernel]"}

Let the _squared exponential_ covariance function be given by
$$
  K(x, x') = \sigma_\mu^2
  \exp \left\{ -\frac{1}{2} \sum_{j=1}^P \left(\frac{x_j - x'_j}{h}\right)^2
    \right\} + \sigma_\delta^2 \,  \delta(x, x') \, .
$$
The constants $(\sigma^2_\mu, \sigma^2_\delta, h)$ are often called
_hyperparameters_ and $\delta(x,x')$ is the Kronecker delta function that takes
the value 1 if $x = x'$ and $0$ otherwise.

a. Let's start with the simple case where $\mathcal{X} = [0,1]$, the unit
   interval. Write a function that simulates a mean-zero Gaussian process on
   $[0,1]$ under the squared exponential covariance function. The function will
   accept as arguments: (1) finite set of points $x_1, \ldots, x_N$ on the unit
   interval; and (2) a triplet $(\sigma^2_\mu, \sigma^2_\delta, h)$. It will
   return the value of the random process at each point: $\mu(x_1), \ldots,
   \mu(x_N)$.

a. Use your function to simulate (and plot) Gaussian processes across a range of
   values for $\sigma^2_\mu$, $\sigma^2_\delta$, and $h$. Try starting with a
   very small value of $\sigma^2_\delta$ (say, $10^{-6}$) and playing around
   with the other two first. On the basis of your experiments, describe the role
   of these three hyperparameters in controlling the overall behavior of the
   random functions that result.

a. Write a function that evaluates the negative marginal log-likelihood of the
   data $\bY$ under a squared exponential kernel with parameters $(\sigma^2_\mu,
   \sigma^2_\delta, h, \sigma^2)$. For reasons we will see in the next part,
   your function should be of the form:
   ```{r, eval = FALSE}
   neg_loglik <- function(theta, sigma_delta_sq, X, y) {
     sigma_mu_sq <- exp(theta[1])
     h <- exp(theta[2])
     sigma_sq <- exp(theta[3])
     ## Your code here:
   }
   ```
   Use this function to evaluate the marginal likelihood of the hyperparameter
   settings $\sigma^2_\mu = 1, \sigma^2_\delta = 0, \sigma^2 = 1, h = 0.2$ on
   the `utilities`, after centering and scaling both `temp` and `gasbill`.
   
a. Use the `optim` function with the `neg_loglik` function to optimize the
   hyperparameters for $(\sigma^2_\mu, \sigma^2, h)$ with the centered/scaled
   `temp` and `gasbill`, with using as initial values the parameters from the
   previous problem. What are the optimal parameter values?

a. Write a function that fits a Gaussian process posterior under this prior 
   distribution to data. The function should be of the form
   ```{r, eval = FALSE}
   fit_gp <- function(X, Y, X_test, sigma_mu_sq, h, sigma_sq, sigma_delta_sq) {
     ## Your code here
     return(list(
            mu_hat = mu_hat, 
            lower = lower,
            upper = upper
     ))
   }
   ```
   where `mu_hat` is the posterior mean of the function evaluated at the points
   in `X_test`, `lower` and `upper` are the endpoints of 95% credible intervals
   intervals. Then use this with the optimal values to construct a scatterplot
   showing the original data, as well as a point estimate and 95% credible band
   for the mean function by fitting to the centered/scaled `temp` and `gasbill`
   (make sure to un-scale the results so that your scatterplot is back on the
   original scale of the data).

a. You might notice that the relationship between $Y_i$ and $X_i$ in this data
   appears to be approximately linear. One way to account for this is to use a
   _linear_ mean function in the Gaussian process model, i.e.,
   $$
     m(x) = \beta_0 + \beta_1 x \, .
   $$
   Since we don't know $\beta_0$ and $\beta_1$, however, we might want to put a
   prior distribution on these as well. Show that if we set $\beta_0 \sim
   \Normal(0, a^2)$ and $\beta_1 \sim \Normal(0, b^2)$, then this is
   _equivalent_ to using the modified covariance function
   $$
     K_{\text{new}}(x, x') = K(x,x') + a^2 + b^2 x x' \, .
   $$
   Equivalently, the covariance matrix for $\bX$ is given by
   $$
     K_{\text{new}}(\bX, \bX) = K(\bX,\bX) + a^2 \, \boldsymbol{J}
       + b^2 \bX\bX^\top \, ,
   $$
   where $\boldsymbol{J} = \boldsymbol 1 \boldsymbol 1^\top$ is a matrix
   consisting of all $1$'s. Now, modify your `fit_gp` function to create a new
   function `fit_gp_linear` that fits a Gaussian process with a linear mean
   function to the `utilities` data (you can fix $a$ and $b$ at somewhat large
   constants for this). Plot the posterior mean and credible band for the
   result. How does this compare to the result from part (d)?

:::

# Curve of Dimensionality

\footnotesize

\alertb{So far:} use local information around $x$ to estimate $\mu(x)$.

\vspace{1em}

\alert{Problem:} In high-dimensional settings, *points are typically all equally
far away from $x$!:

1. Large neighborhood needed around $x$ to have nay data.

2. If neighborhood is large, the bias will be large.

**Optimal MSE scales like $N^{-4/(4 + P)}$.**

# Exercise

\tiny

:::{.myexercise data-latex="[Power Analysis]"}

Supposing that the RMSE scales like $N^{-2/(4+P)}$, how large must $N$ be in
order for us to get an RMSE less than a fixed constant $\delta$? How does this
depend on the dimensionality $P$?

:::

# Exercise

\tiny

:::{.myexercise data-latex="[Geometry]"}

In this exercise, we will demonstrate numerically the point that observations in
a high-dimensional space tend to be far away from one another.

a. Write a function to generate $N$ random data points $X_i$ uniformly
   distribution within a $P$-dimensional hypercube.

a. For each $X_i$, define its _nearest neighbor_ $X_{i'}$ by $i' = \min_{j \ne
   i}\{\|X_i - X_j\|\}$. Then, define the average nearest neighbor distance by
  $$
    NND = \frac{1}{N} \sum_i \|X_i - X_{i'}\|.
  $$
  Write a function that computes the average nearest neighbor distance for a
  given dataset.


a. Generate datasets for different dimensions $P$ ranging from 1 to 50, keeping
   the number of data points $N$ fixed. For each dataset, compute the average
   nearest neighbor distance divided by the average distance overall
   $\frac{1}{N^2} \sum_{i,j} \|X_i - X_j\|$ and store the results in a list.
   Plot the result as a function of $P$. _Explain how the result implies that
   all data points approach an equal amount of "closeness" to any given point as
   the dimension increases._

:::

# Loopholes 

\alert{Dimension reducing structures:}

- Additive functions $\mu(x) = \sum_{j = 1}^P \mu_j(x_j)$.

- Sparsity: $\mu(x)$ depends on $D \ll P$ covariates.

- Covariate structure: maybe $X_i$ is concentrated on a "low-dimensional
  manifold."

# Basis Functions

\footnotesize

A \alert{basis function expansion} for $\mu(x)$ refers to modeling $\mu(x)$ as
$$
  \mu(x) \approx \psi(x)^\top \beta
$$
where $\psi(x) = \{\psi_1(x), \ldots, \psi_B(x)\}$.

\vspace{1em}

\alert{Options:}

- **Polynomials:** $\psi_j(x) = x^j$.
- **Fourier series:** $\psi_j(x) = \cos\left(\frac{k_j\pi x}{L} + \lambda_j \pi\right)$
- **Cubic Splines:** $\psi_j(x) =$ piecewise-cubic function


# Spline Basis

\tiny

```{r spline, echo = FALSE, message=FALSE, fig.cap="A set of cubic spline basis functions."}
plot_spline_basis <- function(num_basis) {
  require(splines)
  require(latex2exp)
  x <- seq(from = 0, to = 1, length = 500)
  knots <- quantile(x, probs = seq(.1, .9, length.out = num_basis + 2))
  X <- ns(x, knots = knots, intercept = FALSE)

  plot(x, X[,1],
    type = 'l',
    ylim = c(0, max(X)),
    xlab = 'x',
    ylab = TeX("$\\psi_j(x)$")
  )
  for(i in 2:num_basis) {
    lines(x, X[,i])
  }
}

plot_spline_basis(10)
```

# A Spline

```{r splinefig, echo = FALSE}
set.seed(888)

x_spline <- seq(from = 0, to = 1, length = 200)
knots_spline <- c(0.3, 0.5, .7)
boundary_knots <- c(.2, .8)
psi_spline <- ns(x_spline,
                 knots = knots_spline, 
                 Boundary.knots = boundary_knots)
## head(psi_spline)

beta_spline <- rnorm(4)
plot(x_spline, as.numeric(psi_spline %*% beta_spline), xlab = "x", 
     ylab = latex2exp::TeX("$\\mu(x)$"))
moo <- sapply(c(knots_spline, boundary_knots), \(y) abline(v = y, lty = 2))
```

# Exercise

\tiny

:::{.myexercise data-latex="[Utilities Again]"}

Consider the `utilities` dataset again.

Write a function that takes as input variables `y` and `x` and a maximal number
of spline basis functions `k_max` and outputs the optimal number of basis
functions to use with the `ns` function in the `splines` package according to
leave-one-out cross-validation. You can do this by fitting a linear model `fit`
for each `k in 1:k_max` and computing the LOOCV as `mean((fit$residuals / (1 -
hatvalues(fit)))^2)`. __How many basis functions is optimal for the `utilities`
dataset?__

:::

# Adaptive Basis Function Expansions

\footnotesize

Can sometimes be advantageous to \alert{learn} a basis from data.

**Examples:**

- \alert{Learning number/location of knots with splines.}
- \alert{Wavelets.}
- \alert{Decision tree boosting.}

# Decision Tree

```{r baseball-tree, echo = FALSE, fig.cap="Salary of MLB Players", out.width="1\\textwidth"}
knitr::include_graphics("Figures/BaseballTree.png")
```

# Exercise

\tiny

:::{.myexercise data-latex="[Basis Expansions in Decision Trees]"}

Using tree-based basis functions as part of a basis function expansion is known
as the _Bayesian additive regression trees_ (BART) framework. BART typically
uses a prior distribution of the form $\eta_{t\ell} \iid \Normal(0,
\sigma^2_\eta / T)$ with the trees $\mathcal T_t$ sampled according to a
[branching process]() prior.

The `boston` dataset is available in the `MASS` package as `boston <-
MASS::Boston`. The dataset contains information about the housing market in the
Boston area in the 1970s, with 506 observations and 14 variables. The dataset is
commonly used to benchmark simple machine learning methods, but the original
motivation was to understand the impact o pollution (specifically, nitrous
oxide) on housing prices. The outcome of interest is `medv`, the median value of
a house in a given census tract. Other variables include `nox` (the predictor of
interest) and confounders such as crime rate (`crim`), distance to employment
centers (`dis`), and the proportion of "lower status" individuals (`lstat`).

a. Load the dataset and install the required package `BART`. 

a. Perform an exploratory analysis of the marginals of the variables. What (if
   any) issues might there be in using the BART model, and how might you correct
   them?

a. Split the data into a training set consisting of 80% observations in the
   training set and 20% of observations in the testing set; we will be using
   this to compare the BART model to Gaussian process regression later.

a. Use the `wbart` function to fit a BART model with `medv` as the outcome. Does
   the method appear to mix well? Justify your answer. __If your approach does
   not mix well in the defaults, make sure to rerun the analysis with a larger
   number of burn-in/save iterations.__

a. A (somewhat crude) measure of the importance of a variable in a BART ensemble
   is the number of times a variable is used to build a decision rule; or
   example, in our MLB tree, the variable `Runs` is used twice, while `SB` is
   used once, indicating that `Runs` may be more important. BART returns the
   total number of uses of each variable at each iteration in the object
   `my_bart_it$varcount`.

   According to this variable importance metric, which variable is used, on
   average, in the most splits?

a. __Optional:__ The `bartMachine` package (which is tricker to install) has a
   function `interaction_investigator` that performs _interaction detection_
   (i.e., it looks or pairs of variables that interact with one another). It
   does this in the same way that `wbart` assesses variable importance: we count
   the number of times two varibales _interact_ in the ensemble, and use this as
   a measure of interaction importance (for example, RBI interacts with SB in
   our MLB tree, arbitrartion status does not interact with SB). Fit the BART
   model using `bartMachine` and use `interaction_investigator` to identify
   important interactions. Are any of them particularly important?

a. Using your train/test split, fit (i) the BART model and (ii) a Gaussian
   process (either modifying your earlier code to allow for multiple predictors
   or using an implementation such as the `bgp` function in the `tgp` package)
   to the training set, and then compute the predicted value of `medv` for the
   testing set. Which of the two methods has a lower value of the MSE $$
   \frac{1}{N_{\text{test}}} \sum_{i \in \text{test set}} (Y_i - \widehat
   Y_i)^2. $$ where the $\widehat Y_i$'s are computed using only the data in the
   training set?

a. Now, take your fitted values from the BART (say, `y_hat_bart`) and GP (say,
   `y_hat_gp`) and fit a linear model of the form `lm(medv ~ y_hat_bart +
   y_hat_gp)` to the test data. Estimate the RMSE of this model using LOOCV. How
   does it compare to the results for the BART/GP model when these methods are
   used by themselves? __Note:__ this strategy is sometimes referred to as
   _model blending_ or _model stacking_.

:::

