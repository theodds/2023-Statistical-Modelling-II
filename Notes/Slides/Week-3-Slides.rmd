---
title: "Week 3 Notes: More Generalized Linear Models and Likelihood Theory"
author: | 
  | Antonio R. Linero
  | University of Texas at Austin
output:
  beamer_presentation:
    includes:
      in_header: "preamble.tex"
classoption:
  - "xcolor=svgnames,dvipsnames,html"      
keep_tex: yes
urlcolor: blue
---

# Goals

1. Learn basic theory underlying GLMs.

1. Learn how to use statistical theory to test simple hypotheses and perform inference.

# Likelihood of a GLM

The likelihood is given by
$$
  L(\beta, \phi)
  =
  \prod_{i = 1}^N \exp\left\{
    \frac{Y_i \theta_i - b(\theta_i)}{\phi / \omega_i} + c(Y_i; \phi / \omega_i)
  \right\},
$$

- $\theta_i \equiv (b')^{-1}(\mu_i)$
- $\mu_i \equiv g^{-1}(X_i^\top \beta)$.


# Score Function

The score function is given by
\begin{align*}
  s(\beta,\phi)
  &=
  \frac{\partial}{\partial \beta} \log L(\beta, \phi)
  \\&=
  \sum_{i=1}^N \frac{\partial}{\partial\beta} \frac{Y_i \theta_i - b(\theta_i)}{\phi / \omega_i} + 
  c(Y_i ; \phi / \omega_i).
  \\&=\underbrace{\sum_{i=1}^N \frac{\omega_i (Y_i - \mu_i) X_i}{\phi V(\mu_i) g'(\mu_i)}}_{\alertb{\text{weighted sum of residuals}}}.
\end{align*}

\alert{The MLE corresponds to the solution to $\widehat\beta$ of $s(\beta, \phi)
= 0$. It is an example of an \emph{$M$-estimator!}}

# The Fisher Information

\tiny

:::{.myexercise data-latex="[Deriving the Fisher Information]"}

We define the *expected* and *observed Fisher Information* to be
$$
  \Fisher(\beta, \phi)
  =
  -\E\left\{ \frac{\partial^2}{\partial\beta\partial\beta^\top} \log L(\beta, \phi) \mid \beta, \phi \right\}.
  \qquad \text{and} \qquad
  \OFisher(\beta,\phi)
  =
  -\frac{\partial^2}{\partial\beta\partial\beta^\top} \log L(\beta, \phi).
$$
  Show that we have
$$
  \langle  \OFisher(\beta,\phi)\rangle_{jk}
  = \frac{1}{\phi} \sum_{i=1}^N X_{ij} X_{ik} \left\{
    \frac{\omega_i}{V(\mu_i) g'(\mu_i)^2} - \frac{\omega_i (Y_i - \mu_i)}{g'(\mu_i)} \left( \frac{\partial}{\partial \mu_i} \frac{1}{V(\mu_i) g'(\mu_i)} \right)
    \right\}
$$
and
$$
  \langle \Fisher(\beta,\phi) \rangle_{jk} =
  \frac{1}{\phi}\sum_{i=1}^N X_{ij} X_{ik}
  \frac{\omega_i}{V(\mu_i) g'(\mu_i)^2}
$$
Show also that $\Fisher(\beta,\phi) = \OFisher(\beta,\phi)$ when the canonical
link is used. Hence we can write
$$
  \Fisher^{-1} = \phi (\bX^\top D \bX)^{-1}
$$

:::

# Aside: Likelihood-Based Inference

- Define $\Data = \{Z_i: i = 1,\ldots, N\}$ iid from $f_{\theta_0}(z)$ 
-  $\{f_\theta : \theta \in \Theta\}$ is a parametric family of densities.
- Likelihood theory quantities:
  \begin{align*}
    \ell(\theta) &= \sum_{i = 1}^N \log f(Z_i \mid \theta),
    \\
    s(\theta) &= \frac{\partial}{\partial \theta} \ell(\theta),
    \\
    \Fisher(\theta) &= - \E\left\{ \frac{\partial^2}{\partial \theta \partial \theta^\top} 
      \ell(\theta) \mid \theta \right\}.
  \end{align*}

# Score Methods

\footnotesize

:::{.myexercise data-latex="[Score Methods]"}

Using the [multivariate central limit
theorem](https://en.wikipedia.org/wiki/Central_limit_theorem#Multidimensional_CLT),
show that
\begin{align*}
  s(\theta_0) \asim \Normal\{0, \Fisher(\theta_0)\},
\end{align*}
but only if we plug in the true value $\theta_0$ _Note:_ this asymptotic
notation means that $X \asim \Normal(\mu, \Sigma)$ if-and-only-if
$\Sigma^{-1/2}(X - \mu) \to \Normal(0, \Identity)$ in distribution.

:::

\alert{What can we do with this?}

# Wald Methods

\footnotesize

:::{.myexercise data-latex="[Wald Methods]"}

Using Taylor's theorem, we have
\begin{align*}
  s(\theta_0)
  =
  s(\widehat \theta) - \OFisher(\theta^\star)(\theta_0 - \widehat \theta)
  = -\OFisher(\theta^\star)(\theta_0 - \widehat\theta).
\end{align*}
where $\theta^\star$ lies on the line segment connecting $\theta_0$ and
$\widehat\theta$. Now, assume that we know somehow that $\widehat\theta$ is a
_consistent_ estimator of $\theta_0$. Show that
\begin{align*}
  \widehat \theta \asim \Normal(\theta_0, \Fisher(\theta_0)^{-1}).
\end{align*}
:::

\alert{What can we do with this?}

# LRT Methods

\footnotesize

:::{.myexercise data-latex="[Likelihood Ratio Methods]"}

Consider the Taylor expansion
\begin{align*}
 \ell(\theta_0)
    =
    \ell(\widehat\theta) + 
      s(\widehat\theta)^\top (\theta_0 - \widehat \theta)
      - \frac{1}{2} 
        (\theta_0 - \widehat \theta)^\top 
        \OFisher(\theta^\star) 
        (\theta_0 - \widehat\theta)
\end{align*}
where $\theta^\star$ lies on the line segment connecting $\widehat\theta$ and $\theta_0$. Show that
\begin{align*}
  -2\{\ell(\theta_0) - \ell(\widehat\theta)\} \to \chi^2_P.
\end{align*}
in distribution, where $P = \dim(\theta)$. Recall here that the $\chi^2_P$ distribution is the distribution of $\sum_{i=1}^P U_i^2$ where $U_1,\ldots,U_P \iid \Normal(0,1)$.

:::

# Wilk

\footnotesize

:::{.mytheorem data-latex="[Wilk's Theorem, label = thm:lrt]"}

Suppose that $\{f_{\theta,\eta} : \theta \in \Theta, \eta \in H\}$ is a
parametric family satisfying certain regularity conditions. Consider the null
hypothesis $H_0: \eta = \eta_0$, let $\widehat \theta_0$ denote the MLE obtained
under the null model, and let $(\widehat \theta, \widehat \eta)$ denote the MLE
under the unrestricted model. Then, if $(\theta_0, \eta_0)$ denote the values of
the parameters that generated the data (so that $H_0$ is true) then
\begin{align*}
  -2\{\ell(\widehat \theta_0, \eta_0) - \ell(\widehat \theta, \widehat \eta)\}
  \asim
  \chi^2_{D}
\end{align*}
where $D = \dim(\eta)$, as the amount of data tends to $\infty$.

:::

- \alert{Note vagueness!}
- \alertb{Great for hypothesis testing!}

# Likelihood-Based Inference for GLMs

\footnotesize

- \alert{Life is easy for Bayesians: all inference flows from posterior.} \pause

- Frequentist inference usually depends on the asymptotics in practice. \pause

:::{.mydefinition data-latex="[Deviance of a GLM]"}

The _saturated model_ has a separate parameter for all unique values of $x$ in
$\Data$:
\begin{align*}
  f(y \mid x, \phi / \omega)
  =
  \exp\left\{
    \frac{y \theta_x - b(\theta_x)}{\phi/\omega} + c(y;\phi/\omega).
  \right\}.
\end{align*}
The _residual deviance_ of a model is defined by
\begin{align*}
  D = -2 \phi\left\{\ell(\widehat \theta) - \ell(\widehat \theta_x) \right\}
\end{align*}
where $\ell(\theta) = \sum_{i=1}^N \dfrac{\omega_i(Y_i\theta_i -
b(\theta_i))}{\phi}$ is the log-likelihood of $\theta$ and $\widehat \theta_{xi}
= (b')^{-1}(Y_i)$. 

The _scaled deviance_ is $D^\star = D / \phi$: it is the LRT statistic for
comparing the model with the saturated model which has the maximal number of
model parameters in the GLM.

:::

# Estimating the Dispersion

:::{.myexercise data-latex="[Estimating the Dispersion]"}

Show that the quantity
$$
  \widetilde \phi = \frac{1}{N} \sum_i \frac{\omega_i (Y_i - \mu_i)^2}{V(\mu_i)}
$$
is unbiased for $\phi$. We don't use $\widetilde\phi$ because we don't know the
$\mu_i$'s, so the modified denominator in $\widehat\phi$ compensates for the
"degrees of freedom" used to estimate $\beta$.

:::

\vspace{1em}

\alertb{In practice:} $\widehat \phi = \frac{1}{N - P} \sum_i \frac{(Y_i -
\widehat \mu_i)^2}{V(\mu_i)}$.


# Analysis of Deviance

\pause

1. \alertb{Goodness-of-fit test with nonparametric alternative:} sometimes,
   $D^\star \asim \chi^2_{N-P}$ under null that model is correct. \pause

2. If model $\mathcal M_0$ is a submodel of $\mathcal M_1$ then the LRT
   statistic for comparing these models is $D^\star_0 - D^\star_1$. Under very
   weak conditions, we have $D^\star_0 - D^\star_1 \asim \chi^2_K$ where $K$ is
   the difference in the number of parameters between the two models.

# More Ships

\tiny

```{r, warning=FALSE}
## Load
ships <- MASS::ships

## Fit GLM (see previous notes)
ships_glm <- glm(
  incidents ~ type + factor(period) + factor(year),
  family = poisson,
  offset = log(service),
  data = dplyr::filter(ships, service > 0)
)

anova(ships_glm, test = "LRT")
```

\alert{Go over table, and goodness of fit.}

# Goodness of Fit Conditions

\footnotesize

- Number of observations is small relative to number of parameters...
- Can be shown that things would be OK if the counts are at least large.

```{r}
print(ships$incidents)
```

# Likelihood-Based Confidence Intervals

\footnotesize

\alert{Confidence Set:}
$$
  \{\beta_{01} :
    \text{The LRT fails to reject $H_0: \beta_0 = \beta_{01}$}\}.
$$
If the LRT has Type I error rate $\alpha$ for all $\beta_{01}$ then the above
set is guaranteed to be a $100(1 - \alpha)\%$ confidence set. 

```{r}
confint(ships_glm)
```

# Drop-1 Tests

\footnotesize

- `anova` does sequential tests.
- `drop1` does "leave one out" tests

```{r}
drop1(ships_glm, test = "LRT")
```


