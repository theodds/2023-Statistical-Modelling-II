---
title: "Week 4 Notes: GLM Theory"
format:
  pdf:
    include-in-header:
      - file: macros.tex
    keep-tex: true
number-depth: 1
number-sections: true
---

This week, we will learn about some of the basic theory underlying GLMs, as well
as how these models tend to be fit in practice. While somewhat abstract, it's
worth noting that the topics presented here have highly pragmatic use cases,
from constructing hypothesis tests to designing inference algorithms, and I
think it's important that one see these things at least once in their life.

# The Likelihood of a GLM

GLMs are fit in `R` using *likelihood based inference*. The likelihood function for a GLM, given data $\Data = \{(Y_i, X_i) : i = 1, \ldots, N\}$ is given by
$$
  L(\beta, \phi)
  =
  \prod_{i = 1}^N \exp\left\{
    \frac{Y_i \theta_i - b(\theta_i)}{\phi / \omega_i} + c(Y_i; \phi / \omega_i)
  \right\},
$$
where we define $\theta_i \equiv (b')^{-1}(\mu_i)$ and $\mu_i \equiv g^{-1}(X_i^\top\beta)$. We can then derive the *score function* $s(\beta, \phi) = \frac{\partial}{\partial \beta} \log L(\beta,\phi)$ as
$$
  s(\beta,\phi)
  =
  \sum_{i=1}^N \frac{\partial}{\partial\beta} \frac{Y_i \theta_i - b(\theta_i)}{\phi / \omega_i} + 
  c(Y_i ; \phi / \omega_i).
$$
Again, we will write $\frac{\partial}{\partial\beta} F(\beta)$ for the gradient of $F$ and $\frac{\partial^2}{\partial\beta\partial\beta^\top} F(\beta)$ for the Hessian matrix.

:::{.myexercise data-latex="[Deriving the Score]"}

Using the chain rule $\frac{\partial}{\partial\beta} = \frac{\partial}{\partial \theta} \times \frac{\partial \theta}{\partial\mu} \times \frac{\partial \mu}{\partial\beta}$, show that
$$
  s(\beta, \phi)
  =
  \sum_{i=1}^N \frac{\omega_i (Y_i - \mu_i) X_i}{\phi V(\mu_i) g'(\mu_i)}.
$$
Show also that, for the canonical link, we have $g'(\mu_i) = V(\mu_i)^{-1}$ so that this reduces to
$$
  s(\beta, \phi)
  =
  \sum_{i=1}^N \frac{\omega_i (Y_i - \mu_i) X_i}{\phi}.
$$
**Hint:** recall that $\frac{d}{dx}g^{-1}(x) = \frac{1}{g'\{g^{-1}(x)\}}$.

:::

Note for posterity that the MLE is justified, in large part, because it is the
sample solution $s(\widehat \beta, \phi) = 0$ to the population-level estimating
equation $\int s(\beta, \phi) \, f_0(\by \mid \bX) d\by = 0$ (here, $\bX$
denotes the design matrix and $f_0$ denotes the true conditional density of $\bY
= (Y_1, \ldots, Y_N$). Also note that this estimating equation holds *even when
the model is misspecified!* We only need the mean structure (not the exponential
dispersion family) for the population-level estimating equation to hold, which
suggests that the MLE $\widehat \beta$ might still be a good estimator even when
the model is misspecified. More on this when we study $M$-estimation.

:::{.myexercise data-latex="[Deriving the Fisher Information]"}

We define the *Fisher Information* to be
$$
  \Fisher(\beta, \phi)
  =
  -\E\left\{ \frac{\partial^2}{\partial\beta\partial\beta^\top} \log L(\beta, \phi) \mid \beta, \phi \right\}.
$$
The Fisher information plays an important role in inference for GLMs. The "observed" Fisher information is also used,
$$
  \OFisher(\beta,\phi)
  =
  -\frac{\partial^2}{\partial\beta\partial\beta^\top} \log L(\beta, \phi).
$$
In addition to being easier to evaluate, using $\OFisher$ has been argued to be the right-thing-to-do\texttrademark. In any case, show that
$$
  \langle  \OFisher(\beta,\phi)\rangle_{jk}
  = \frac{1}{\phi} \sum_{i=1}^N X_{ij} X_{ik} \left\{
    \frac{\omega_i}{V(\mu_i) g'(\mu_i)^2} - \frac{\omega_i (Y_i - \mu_i)}{g'(\mu_i)} \left( \frac{\partial}{\partial \mu_i} \frac{1}{V(\mu_i) g'(\mu_i)} \right)
    \right\}
$$
and
$$
  \langle \Fisher(\beta,\phi) \rangle_{jk} =
  \frac{1}{\phi}\sum_{i=1}^N X_{ij} X_{ik}
  \frac{\omega_i}{V(\mu_i) g'(\mu_i)^2}
$$
where $\langle A \rangle_{ij}$ denotes the $(i,j)^{\text{th}}$ element of the matrix $A$. Show also that $\Fisher(\beta,\phi) = \OFisher(\beta,\phi)$ when the canonical link is used.

:::

From the above exercise, notice that the Fisher information has the familiar form
\begin{align*}
  \Fisher^{-1} = \phi (\bX^\top D \bX)^{-1}
\end{align*}
where $D$ is a diagonal matrix with entries $\omega_i / \{V(\mu_i) g(\mu_i)^2\}$. Similarly, we can write $\OFisher^{-1} = \phi(\bX^\top \widetilde D \bX)^{-1}$  for some diagonal matrix $\widetilde D$. Compare this with the linear model,  which has inverse Fisher information $\Fisher^{-1} = \sigma^2 (X^\top X)^{-1}$.

# Aside: Likelihood-Based Inference

In this section, we will briefly refresh our memories on the theory underlying
likelihood methods. For simplicity, consider data $\Data = \{Z_i : i = 1,
\ldots, N\}$ with $Z_i$'s iid according to some density $f(z \mid \theta_0)$
where $\{f(\cdot \mid \theta : \theta \in \Theta)\}$ is a family of
distributions satisfying some (unstated) regularity conditions. We define the
log-likelihood, score, and Fisher information with the equations
\begin{align*}
  \ell(\theta) = \sum_{i = 1}^N \log f(Z_i \mid \theta),
  \quad
  s(\theta) = \frac{\partial}{\partial \theta} \ell(\theta),
  \quad \text{and} \quad
  \Fisher(\theta) = - \E\left\{ \frac{\partial^2}{\partial \theta \partial \theta^\top} \ell(\theta) \mid \theta \right\}.
\end{align*}
The following identities are fundamental to likelihood inference
\begin{align*}
  \E\{s(\theta) \mid \theta\} = \zeros,
  \qquad \text{and} \qquad
  \Var\{s(\theta) \mid \theta\} = \Fisher(\theta).
\end{align*}
We will study three types of methods for constructing inference procedures from
the likelihood: Score methods, Wald methods, and likelihood ratio (LR) methods.

<!--

# :::{.exercise #notes-glm-score}

# Using the [multivariate central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem#Multidimensional_CLT), show that
# $$
#   s(\theta_0) \asim \Normal\{0, \Fisher(\theta_0)\},
# $$
# but only if we plug in the true value $\theta_0$ _Note:_ this asymptotic notation means that $X \asim \Normal(\mu, \Sigma)$ if-and-only-if $\Sigma^{-1/2}(X - \mu) \to \Normal(0, \Identity)$ in distribution.

# :::

# :::{.exercise #notes-glm-wald}

# Using Taylor's theorem, we have
# $$
#   s(\theta_0)
#   =
#   s(\widehat \theta) - \OFisher(\theta^\star)(\theta_0 - \widehat \theta)
#   = -\OFisher(\theta^\star)(\theta_0 - \widehat\theta).
# $$
# where $\theta^\star$ lies on the line segment connecting $\theta_0$ and $\widehat\theta$. Now, assume that we know somehow that $\widehat\theta$ is a _consistent_ estimator of $\theta_0$. Show that
# $$
#   \widehat \theta \asim \Normal(\theta_0, \Fisher(\theta_0)^{-1}).
# $$
# __Note:__ you do not need to give a completely rigorous proof. In particular, you can assume that, if $\theta_N \to \theta_0$, then $\frac{1}{N} \OFisher(\theta_N) \to i(\theta_0)$ where
# $$
#   i(\theta_0)
#   = -\E\left\{ \frac{\partial^2}{\partial\theta\partial\theta^\top} \log f(Z_i \mid \theta)  \right\}
#   =
#   \Fisher(\theta_0) / N.
# $$

# :::

# :::{.exercise #notes-glm-lrt}

# Consider the Taylor expansion
# $$
#  \ell(\theta_0)
#     =
#     \ell(\widehat\theta) + 
#       s(\widehat\theta)^\top (\theta_0 - \widehat \theta)
#       - \frac{1}{2} 
#         (\theta_0 - \widehat \theta)^\top 
#         \OFisher(\theta^\star) 
#         (\theta_0 - \widehat\theta)
# $$
# where $\theta^\star$ lies on the line segment connecting $\widehat\theta$ and $\theta_0$. Using Exercise \@ref(exr:notes-glm-wald), show that
# $$
#   -2\{\ell(\theta_0) - \ell(\widehat\theta)\} \to \chi^2_P.
# $$
# in distribution, where $P = \dim(\theta)$. Recall here that the $\chi^2_P$ distribution is the distribution of $\sum_{i=1}^P U_i^2$ where $U_1,\ldots,U_P \iid \Normal(0,1)$.

# :::

# Exercise \@ref(exr:notes-glm-wald) provides the basis for "Wald" methods, while
# Exercise \@ref{exr:notes-glm-score} provides the basis for "score" methods, and
# Exercise \@ref{ex:notes-glm-lrt} provides the basis for "likelihood ratio" (LR) methods.

# More generally one can show, using the same sorts of Taylor expansions used above, the following result.

# :::{.theorem #lrt name="Wilk's Theorem"}

# Suppose that $\{f_{\theta,\eta} : \theta \in \Theta, \eta \in H\}$ is a parametric family satisfying certain regularity conditions. Consider the null hypothesis $H_0: \eta = \eta_0$, let $\widehat \theta_0$ denote the MLE obtained under the null model, and let $(\widehat \theta, \widehat \eta)$ denote the MLE under the unrestricted model. Then, if $(\theta_0, \eta_0)$ denote the values of the parameters that generated the data (so that $H_0$ is true) then
# $$
#   -2\{\ell(\widehat \theta_0, \eta_0) - \ell(\widehat \theta, \widehat \eta)\}
#   \asim
#   \chi^2_{D}
# $$
# where $D = \dim(\eta)$, as the amount of data tends to $\infty$.

# :::

# The statement of the result above is deliberately vague; the regularity conditions are unstated, and what it means for the "amount of data" to tend to $\infty$ is not made precise. If our data is of the form $\Data = \{(X_i, Y_i) : i = 1,\ldots,N\}$ then taking $N \to \infty$ will suffice, but there are other situations where the result will hold even if $N$ is fixed (with the data "tending to infinity" in other ways). An example in which this is the case is analysis of contingency tables, where $N$ denotes the number of cells. Another important requirement to get a result like this is that we should have the dimension of the parameters fixed, which is sometimes not the case even for old-school GLMs.

# The above result will let us do things like perform hypothesis tests and construct confidence intervals.

# __Note:__ It is also possible to extend the score method to the case with parameter constraints, but we don't have time to do so. My experience is that, for the most part, LR methods tend to be better than score methods (both are better than Wald methods), although this isn't universally true. Notice that LR methods require fitting both a constrained and unconstrained model. It turns out that score methods only require fitting the constrained model, which can occasionally be useful if the unconstrained model is difficult to fit.

# ## Likelihood-Based Inference for GLMs


# Bayesian inference for GLMs is quite straight-forward --- just fit the model with `stan_glm`, get the posterior samples, and interpret the results as you normally would.

# For Frequentist inference, we can use likelihood-based methods to do things like conduct hypothesis tests. A convenient quantity to use is the _deviance_ of a GLM.

# :::{.definition name="Deviance of a GLM"}

# Let $\Data = \{(Y_i, X_i) : i = 1 ,\ldots, N\}$ be modeled with a GLM in the exponential dispersion family with canonical parameters $\theta_i = (b')^{-1}(\mu_i) = (b')^{-1}(g^{-1}(X_i^\top\beta))$. Let $\theta = (\theta_1, \ldots, \theta_N)$ and let $\widehat \theta = (\widehat \theta_1, \ldots, \widehat \theta_N)$ be the MLE of the $\theta$'s under our model. We define the _saturated model_ as the model which has a separate parameter for all unique values of $x$ in $\Data$:
# $$
#   f(y \mid x, \phi / \omega)
#   =
#   \exp\left\{
#     \frac{y \theta_x - b(\theta_x)}{\phi/\omega} + c(y;\phi/\omega).
#   \right\}.
# $$
# The _residual deviance_ of a model is defined by
# $$
#   D = -2 \phi\left\{\ell(\widehat \theta) - \ell(\widehat \theta_x) \right\}
# $$
# where $\ell(\theta) = \sum_{i=1}^N \dfrac{\omega_i(Y_i\theta_i - b(\theta_i))}{\phi}$ is the log-likelihood of $\theta$. The _scaled deviance_ is $D^\star = D / \phi$. __Note:__ if $\phi$ is unknown, we generally replace $\phi$ with an estimate $\widehat \phi$ given by
# $$
#   \widehat \phi = \frac{1}{N-P} \sum_{i=1}^N \frac{\omega_i (Y_i - \widehat \mu_i)^2}{V(\widehat \mu_i)}.
# $$

# :::

# :::{.exercise}

# Show that the quantity
# $$
#   \widetilde \phi = \frac{1}{N} \sum_i \frac{\omega_i (Y_i - \mu_i)^2}{V(\mu_i)}
# $$
# is unbiased for $\phi$. We don't use $\widetilde\phi$ because we don't know the $\mu_i$'s, so the modified denominator in $\widehat\phi$ compensates for the "degrees of freedom" used to estimate $\beta$. 

# :::

# __In words:__ the scaled deviance is the LRT statistic for comparing the model with the saturated model which has the maximal number of model parameters in the GLM.

# If it were up to me, I would only ever talk about the scaled deviance, since the scaled deviance is interpretable as an LRT statistic. Unfortunately, the software output of `R` returns the deviance, and $D \ne D^\star$. Conveniently, however, $D = D^\star$ for the Poisson and Binomial GLMs.


# **What is the deviance good for?** The deviance is commonly used for two purposes. 

# 1. It can be used as a goodness-of-fit statistic, testing to see whether the model under consideration would be rejected in favor of the saturated model. _From a modern perspective, this might be construed as a test of the parametric model against a nonparametric alternative, without making any smoothness assumptions on $\theta_x$._ This is a little bit tricky, however, since the saturated model has $P = N$ parameters, so the usual asymptotics (where $N$ is large relative to $P$) do not apply. In certain situations, however, one can show that $D^\star \asim \chi^2_{N-P}$, as would be suggested by a naive application of Theorem \@ref(thm:lrt).

# 2. If model $\mathcal M_0$ is a submodel of $\mathcal M_1$ then the LRT statistic for comparing these models is $D^\star_0 - D^\star_1$. Under very weak conditions, we have $D^\star_0 - D^\star_1 \asim \chi^2_K$ where $K$ is the difference in the number of parameters between the two models.

# For example, we can easily perform the LRT in Theorem \@ref(thm:lrt) using the `anova` function.
# ```{r, warning=FALSE}
# anova(ships_glm, test = "LRT")
# ```
# This table gives an _analysis of deviance_, which should feel quite similar to the analysis of variance that you are already familiar with.

# (a) The `NULL` residual deviance gives the deviance of an intercept-only model. 

# (a) The `type` residual deviance gives the deviance of a model with `type` and an intercept.

# (a) The `period` residual deviance gives the deviance of a model with `type`,
# `period`, and an intercept.

# (a) The `year` residual deviance gives the deviance of a model with all terms
# included in the model.

# (a) The columns `Df` and `Deviance` give the degrees of freedom and the LRT statistic that compares _the model including all terms up-to-and-including the current line with the model including all terms up-to-but-not-including the current line_. For example, the `Deviance` corresponding to `factor(period)` tests a model with terms `~ intercept + type` against a model with terms `~ intercept + type + period`.

# The results of the analysis of deviance point clearly to the relevance of the individual terms. Unlike the output of `summary`, this provides the likelihood ratio test rather than Wald tests for the parameters, and the different variables have been helpfully grouped together (i.e., we have a single term of `type` with four degrees of freedom, rather than four separate coefficients).

# The last residual deviance for the full model is $38.695$, with a (naive) null distribution of $\chi^2_{25}$. This corresponds to a $P$-value of
# ```{r}
# pchisq(38.695, df = 25, lower.tail = FALSE)
# ```
# This gives some evidence that the model lacks fit, and the model could be disproved in favor of the saturated model. As mentioned above, however, the deviance may not be close to a $\chi^2_{N-P}$ distribution in this case. For Poisson data, the key is that the individual counts for each observation should be largeish. But
# ```{r}
# print(ships$incidents)
# ```
# Hence, use of the deviance in this situation as a goodness of fit test seems questionable.

# We can also construct likelihood-based confidence intervals for the parameters. If I want a confidence interval for (say) $\beta_1$, I can get one by _inverting_ the test $H_0: \beta_1 = \beta_{01}$ to get a confidence set
# $$
#   \{\beta_{01} :
#     \text{The LRT fails to reject $H_0: \beta_0 = \beta_{01}$}\}.
# $$
# If the LRT has Type I error rate $\alpha$ for all $\beta_{01}$ then the above set is guaranteed to be a $100(1 - \alpha)\%$ confidence set. This is implemented in `R` with the function `confint`:
# ```{r}
# confint(ships_glm)
# ```
# **In general, you should use `anova` and `confint` rather than the output of
# `summary`.**

# Lastly, rather than doing sequential tests like `anova` does, you can do a "leave-one-out" test using `drop1` as follows.
# ```{r}
# drop1(ships_glm, test = "LRT")
# ```

# This tests each model term under the assumption that the other terms are in the model.

# :::{.exercise}

# Try these ideas out on the Challenger dataset. How do your conclusions differ (nor not differ) from the Bayesian analysis?

# :::

-->
